# üöÄ Learning Roadmap: Building a Simplified RAG Chat Application

A comprehensive guide to building a minified version of an AI-powered RAG (Retrieval-Augmented Generation) system for document analysis with chat interface, based on the Regulaido project.

## üìã Table of Contents

- [Overview](#overview)
- [What You'll Build](#what-youll-build)
- [Prerequisites](#prerequisites)
- [Phase 1: Foundation (2-3 weeks)](#phase-1-foundation-2-3-weeks)
- [Phase 2: Backend & Database (2-3 weeks)](#phase-2-backend--database-2-3-weeks)
- [Phase 3: Authentication (1-2 weeks)](#phase-3-authentication-1-2-weeks)
- [Phase 4: AI Integration Basics (2-3 weeks)](#phase-4-ai-integration-basics-2-3-weeks)
- [Phase 5: RAG Implementation (3-4 weeks)](#phase-5-rag-implementation-3-4-weeks)
- [Phase 6: Real-time Features (1-2 weeks)](#phase-6-real-time-features-1-2-weeks)
- [Phase 7: File Storage (1 week)](#phase-7-file-storage-1-week)
- [Phase 8: Polish & Deploy (1-2 weeks)](#phase-8-polish--deploy-1-2-weeks)
- [Timeline & Resources](#timeline--resources)

---

## Overview

This roadmap guides you through building a document Q&A system powered by AI, where users can:
- Upload PDF documents
- Ask questions about the documents
- Receive AI-generated answers based on the document content (RAG)
- Organize documents in projects
- View chat history

**Estimated Timeline**: 14-20 weeks (3-5 months) at 10-15 hours/week

---

## What You'll Build

### MVP Features
1. ‚úÖ User authentication (email/password)
2. ‚úÖ Create projects
3. ‚úÖ Upload PDF files to projects
4. ‚úÖ Process PDFs ‚Üí extract text ‚Üí vectorize
5. ‚úÖ Chat interface per project
6. ‚úÖ RAG-powered responses (AI answers based on uploaded docs)
7. ‚úÖ View chat history

### Nice-to-Have Features
8. Real-time file processing status
9. Multiple file support per project
10. Citation/source highlighting in responses
11. Export chat as PDF/Word
12. User settings (system prompt customization)

---

## Prerequisites

- Basic understanding of JavaScript/TypeScript
- Familiarity with HTML/CSS
- Node.js installed (v18+)
- PostgreSQL installed (or Docker)
- OpenAI API key (required for AI features)

---

## Phase 1: Foundation (2-3 weeks)

### Core Technologies

1. **TypeScript Fundamentals**
   - Type system, interfaces, generics
   - Modern ES6+ features
   - üìö [TypeScript Handbook](https://www.typescriptlang.org/docs/handbook/)

2. **SvelteKit Basics**
   - Svelte 5 syntax (runes: `$state`, `$derived`, `$effect`)
   - Routing and layouts
   - Load functions and form actions
   - üìö [SvelteKit Tutorial](https://learn.svelte.dev/)

3. **CSS with Tailwind**
   - Utility-first CSS
   - Responsive design
   - üé® Tailwind CSS + DaisyUI for components

### Mini-Project

Build a basic SvelteKit app with:
- Home page, about page
- Navigation with layouts
- Simple form with validation
- Responsive styling

### Setup Commands

```bash
# Create new SvelteKit project
npm create svelte@latest my-rag-app
cd my-rag-app
npm install

# Install Tailwind & DaisyUI
npm install -D tailwindcss postcss autoprefixer daisyui
npx tailwindcss init -p

# Install TypeScript types
npm install -D @types/node
```

---

## Phase 2: Backend & Database (2-3 weeks)

### Technologies

1. **PostgreSQL**
   - SQL basics (CRUD operations)
   - Relationships (one-to-many, many-to-many)
   - Indexes
   - üìö [PostgreSQL Tutorial](https://www.postgresqltutorial.com/)

2. **Prisma ORM**
   - Schema definition
   - Migrations
   - Queries and relations
   - üìö [Prisma Docs](https://www.prisma.io/docs)

3. **API Design**
   - REST conventions
   - HTTP methods and status codes
   - Request validation with Zod

### Database Schema (Simplified)

```prisma
// prisma/schema.prisma

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

generator client {
  provider = "prisma-client-js"
}

model User {
  id        String    @id @default(cuid())
  email     String    @unique
  name      String?
  password  String
  projects  Project[]
  chats     Chat[]
  files     File[]
  created   DateTime  @default(now())
}

model Project {
  id          String   @id @default(cuid())
  name        String
  description String?
  ownerId     String
  owner       User     @relation(fields: [ownerId], references: [id], onDelete: Cascade)
  files       File[]
  chats       Chat[]
  created     DateTime @default(now())
}

model File {
  id          String   @id @default(cuid())
  name        String
  blobName    String   // Stored filename
  projectId   String
  project     Project  @relation(fields: [projectId], references: [id], onDelete: Cascade)
  status      String   @default("pending") // "pending" | "processing" | "success" | "failed"
  created     DateTime @default(now())
}

model Chat {
  id          String    @id @default(cuid())
  name        String?
  projectId   String
  project     Project   @relation(fields: [projectId], references: [id], onDelete: Cascade)
  messages    Message[]
  created     DateTime  @default(now())
}

model Message {
  id      String   @id @default(cuid())
  chatId  String
  chat    Chat     @relation(fields: [chatId], references: [id], onDelete: Cascade)
  role    String   // "user" | "assistant"
  content String
  created DateTime @default(now())
}
```

### Setup Commands

```bash
# Install Prisma
npm install @prisma/client
npm install -D prisma

# Initialize Prisma
npx prisma init

# Create and run migrations
npx prisma migrate dev --name init

# Generate Prisma client
npx prisma generate
```

### Mini-Project

Build a task manager with:
- User registration/login
- Create/read/update/delete tasks
- API endpoints with validation
- Database persistence

---

## Phase 3: Authentication (1-2 weeks)

### Technologies

**Simple Option (Recommended for Learning)**: Auth.js with GitHub or Google OAuth

**Alternative**: Email/password with bcrypt

### Implementation

```typescript
// src/hooks.server.ts

import type { Handle } from '@sveltejs/kit';
import { prisma } from '$lib/server/prisma';

export const handle: Handle = async ({ event, resolve }) => {
  // Get session token from cookie
  const sessionId = event.cookies.get('session');
  
  if (sessionId) {
    // Fetch user from database
    const user = await prisma.user.findFirst({
      where: { sessionId }
    });
    
    if (user) {
      event.locals.user = user;
    }
  }
  
  return resolve(event);
};
```

### Mini-Project

Add authentication to your task manager:
- Login/logout flow
- Protected API routes
- User-specific data filtering

---

## Phase 4: AI Integration Basics (2-3 weeks)

### Technologies

1. **OpenAI API**
   - Chat completions
   - Streaming responses
   - Token management

### Basic Implementation

```typescript
// src/lib/server/openai.ts

import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

export async function chatCompletion(messages: Array<{
  role: 'system' | 'user' | 'assistant';
  content: string;
}>) {
  const response = await openai.chat.completions.create({
    model: 'gpt-4o-mini', // Use cheaper model for learning
    messages,
    max_tokens: 500,
    temperature: 0.7
  });
  
  return response.choices[0].message.content;
}
```

### API Endpoint Example

```typescript
// src/routes/api/chat/[id]/message/+server.ts

import { json } from '@sveltejs/kit';
import { chatCompletion } from '$lib/server/openai';
import { prisma } from '$lib/server/prisma';

export async function POST({ params, request, locals }) {
  const { message } = await request.json();
  const chatId = params.id;
  
  // Save user message
  await prisma.message.create({
    data: {
      chatId,
      role: 'user',
      content: message
    }
  });
  
  // Get chat history
  const messages = await prisma.message.findMany({
    where: { chatId },
    orderBy: { created: 'asc' }
  });
  
  // Generate AI response
  const aiResponse = await chatCompletion([
    { role: 'system', content: 'You are a helpful assistant.' },
    ...messages.map(m => ({ 
      role: m.role as 'user' | 'assistant', 
      content: m.content 
    })),
  ]);
  
  // Save assistant message
  await prisma.message.create({
    data: {
      chatId,
      role: 'assistant',
      content: aiResponse
    }
  });
  
  return json({ response: aiResponse });
}
```

### Mini-Project

Build a simple chatbot:
- Chat interface
- Send messages to OpenAI
- Display responses
- Maintain conversation history

---

## Phase 5: RAG Implementation (3-4 weeks)

### Technologies

1. **Vector Database**
   - **Simple**: Pinecone (free tier)
   - **Self-hosted**: Qdrant or ChromaDB
   
2. **Document Processing**
   - **Library**: `pdf-parse` or `pdf.js`
   - Text chunking strategies

3. **Embeddings**
   - OpenAI embeddings (`text-embedding-3-small`)

### RAG Architecture

```typescript
// src/lib/server/rag.ts

import OpenAI from 'openai';
import { Pinecone } from '@pinecone-database/pinecone';
import pdf from 'pdf-parse';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
const index = pinecone.index('documents');

// 1. Process and vectorize document
export async function processDocument(
  fileBuffer: Buffer, 
  fileId: string
) {
  // Extract text from PDF
  const data = await pdf(fileBuffer);
  const text = data.text;
  
  // Split into chunks (500-1000 characters each)
  const chunks = splitIntoChunks(text, 1000);
  
  // Generate embeddings for each chunk
  const embeddings = await Promise.all(
    chunks.map(async (chunk, index) => {
      const response = await openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: chunk
      });
      
      return {
        id: `${fileId}-chunk-${index}`,
        values: response.data[0].embedding,
        metadata: { text: chunk, fileId }
      };
    })
  );
  
  // Store in vector database
  await index.upsert(embeddings);
}

// 2. Query with RAG
export async function queryWithRAG(
  question: string, 
  projectId: string
) {
  // Generate embedding for question
  const questionEmbedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: question
  });
  
  // Find similar chunks
  const queryResponse = await index.query({
    vector: questionEmbedding.data[0].embedding,
    topK: 3,
    includeMetadata: true,
    filter: { projectId }
  });
  
  // Build context from relevant chunks
  const context = queryResponse.matches
    .map(match => match.metadata?.text)
    .join('\n\n');
  
  // Generate response with context
  const response = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [
      {
        role: 'system',
        content: `Answer the question based on the following context. If you cannot answer based on the context, say so.

Context:
${context}`
      },
      {
        role: 'user',
        content: question
      }
    ],
    max_tokens: 500
  });
  
  return response.choices[0].message.content;
}

// Helper function to split text into chunks
function splitIntoChunks(text: string, chunkSize: number): string[] {
  const chunks: string[] = [];
  const sentences = text.match(/[^.!?]+[.!?]+/g) || [];
  
  let currentChunk = '';
  
  for (const sentence of sentences) {
    if ((currentChunk + sentence).length > chunkSize) {
      if (currentChunk) chunks.push(currentChunk.trim());
      currentChunk = sentence;
    } else {
      currentChunk += sentence;
    }
  }
  
  if (currentChunk) chunks.push(currentChunk.trim());
  
  return chunks;
}
```

### Setup Commands

```bash
# Install dependencies
npm install pdf-parse @pinecone-database/pinecone

# Set up environment variables
echo "PINECONE_API_KEY=your_key_here" >> .env
echo "PINECONE_ENVIRONMENT=your_env_here" >> .env
```

### Mini-Project

Build a document Q&A system:
- Upload PDFs
- Process and vectorize documents
- Ask questions about documents
- Receive AI-generated answers with context

---

## Phase 6: Real-time Features (1-2 weeks)

### Technologies

**Server-Sent Events (SSE)** - Perfect for one-way server-to-client updates

### Implementation

```typescript
// src/routes/api/sse/+server.ts

export async function GET({ locals }) {
  const userId = locals.user?.id;
  
  const stream = new ReadableStream({
    start(controller) {
      const encoder = new TextEncoder();
      
      const send = (event: string, data: any) => {
        controller.enqueue(
          encoder.encode(`event: ${event}\ndata: ${JSON.stringify(data)}\n\n`)
        );
      };
      
      // Send initial connection message
      send('connected', { message: 'Connected to SSE' });
      
      // Poll database for updates (simplified example)
      const interval = setInterval(async () => {
        // Check for new messages, file status updates, etc.
        const updates = await checkForUpdates(userId);
        if (updates.length > 0) {
          send('update', updates);
        }
      }, 2000);
      
      // Cleanup on close
      return () => {
        clearInterval(interval);
        controller.close();
      };
    }
  });
  
  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  });
}
```

### Client-Side Usage

```svelte
<script lang="ts">
  import { onMount } from 'svelte';
  
  let updates = $state<any[]>([]);
  
  onMount(() => {
    const eventSource = new EventSource('/api/sse');
    
    eventSource.addEventListener('update', (event) => {
      const data = JSON.parse(event.data);
      updates.push(data);
    });
    
    eventSource.onerror = () => {
      console.error('SSE connection error');
    };
    
    return () => {
      eventSource.close();
    };
  });
</script>

{#each updates as update}
  <div>{update.message}</div>
{/each}
```

### Mini-Project

Add real-time updates:
- File processing status
- New chat messages
- Typing indicators

---

## Phase 7: File Storage (1 week)

### Technologies

**Options:**
1. **AWS S3** (recommended for production)
2. **Azure Blob Storage** (like the original app)
3. **Local storage** (for development)

### Simple File Upload Implementation

```typescript
// src/routes/api/upload/+server.ts

import { writeFile } from 'fs/promises';
import { join } from 'path';

export async function POST({ request, locals }) {
  const formData = await request.formData();
  const file = formData.get('file') as File;
  const projectId = formData.get('projectId') as string;
  
  if (!file) {
    return new Response('No file provided', { status: 400 });
  }
  
  // Generate unique filename
  const filename = `${Date.now()}-${file.name}`;
  const filepath = join('static', 'uploads', filename);
  
  // Save file
  const buffer = Buffer.from(await file.arrayBuffer());
  await writeFile(filepath, buffer);
  
  // Save to database
  const dbFile = await prisma.file.create({
    data: {
      name: file.name,
      blobName: filename,
      projectId,
      status: 'pending'
    }
  });
  
  // Process file asynchronously
  processDocument(buffer, dbFile.id).catch(console.error);
  
  return json({ fileId: dbFile.id });
}
```

### Mini-Project

Implement file upload:
- Upload endpoint
- Store files locally or in cloud
- Generate download URLs
- Delete files

---

## Phase 8: Polish & Deploy (1-2 weeks)

### Technologies

1. **State Management**: TanStack Query (Svelte Query)
2. **Error Handling**: Global error boundaries
3. **Deployment**: Vercel, Railway, or Render

### TanStack Query Example

```typescript
// src/routes/+page.svelte
<script lang="ts">
  import { createQuery } from '@tanstack/svelte-query';
  
  const projectsQuery = createQuery(() => ({
    queryKey: ['projects'],
    queryFn: async () => {
      const res = await fetch('/api/projects');
      return res.json();
    }
  }));
</script>

{#if $projectsQuery.isLoading}
  <p>Loading...</p>
{:else if $projectsQuery.error}
  <p>Error: {$projectsQuery.error.message}</p>
{:else}
  {#each $projectsQuery.data as project}
    <div>{project.name}</div>
  {/each}
{/if}
```

### Deployment to Vercel

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
vercel

# Set environment variables
vercel env add DATABASE_URL
vercel env add OPENAI_API_KEY
vercel env add PINECONE_API_KEY
```

---

## Timeline & Resources

### Estimated Timeline

| Phase | Duration | Focus |
|-------|----------|-------|
| Phase 1 | 2-3 weeks | Foundation (TypeScript, SvelteKit, Tailwind) |
| Phase 2 | 2-3 weeks | Backend (PostgreSQL, Prisma, API design) |
| Phase 3 | 1-2 weeks | Authentication |
| Phase 4 | 2-3 weeks | AI Integration (OpenAI API) |
| Phase 5 | 3-4 weeks | RAG Implementation (Vector DB, embeddings) |
| Phase 6 | 1-2 weeks | Real-time Features (SSE) |
| Phase 7 | 1 week | File Storage |
| Phase 8 | 1-2 weeks | Polish & Deploy |
| **Total** | **14-20 weeks** | **3-5 months at 10-15 hours/week** |

### Essential Libraries

```json
{
  "dependencies": {
    "@prisma/client": "latest",
    "@sveltejs/kit": "latest",
    "@tanstack/svelte-query": "latest",
    "@pinecone-database/pinecone": "latest",
    "svelte": "^5.0.0",
    "openai": "latest",
    "zod": "latest",
    "tailwindcss": "latest",
    "daisyui": "latest",
    "pdf-parse": "latest"
  },
  "devDependencies": {
    "@types/node": "latest",
    "typescript": "latest",
    "prisma": "latest",
    "vite": "latest"
  }
}
```

### Learning Resources

1. **SvelteKit**: [learn.svelte.dev](https://learn.svelte.dev)
2. **Prisma**: [prisma.io/docs](https://www.prisma.io/docs)
3. **OpenAI**: [platform.openai.com/docs](https://platform.openai.com/docs)
4. **PostgreSQL**: [postgresqltutorial.com](https://www.postgresqltutorial.com/)
5. **Pinecone**: [docs.pinecone.io](https://docs.pinecone.io/)
6. **RAG Concepts**: Search "Retrieval Augmented Generation tutorial"

---

## Key Differences: Full App vs. Minified Version

| Feature | Original App (Regulaido) | Minified Version |
|---------|----------|------------------|
| **Auth** | Azure AD (Enterprise SSO) | Simple OAuth or email/password |
| **AI Provider** | Azure OpenAI + Google Gemini | OpenAI API directly |
| **Vector DB** | Custom solution | Pinecone or simple in-memory |
| **Real-time** | Complex SSE + message queues | Basic SSE |
| **File Processing** | Async with Workcraft queue system | Direct processing |
| **Analytics** | Complex tracking with experiments | Basic logging |
| **User Roles** | 7 different roles + permissions | 2 roles (user/admin) |
| **A/B Testing** | Full experiment system | Not included |
| **File Storage** | Azure Blob Storage | AWS S3 or local storage |
| **Monitoring** | Sentry, Langfuse integration | Simple error logging |

---

## Tips for Success

1. **Build progressively**: Complete each phase before moving to the next
2. **Create mini-projects**: Each phase should produce a working demo
3. **Start simple**: Use simpler alternatives first (local storage before cloud, email auth before OAuth)
4. **Test early**: Write basic tests as you go
5. **Document learnings**: Keep notes on challenges and solutions
6. **Join communities**: SvelteKit Discord, OpenAI forums
7. **Use cheaper models**: `gpt-4o-mini` is perfect for learning (80% cheaper than GPT-4)

---

## What's Next?

After completing this roadmap, you can add:

- üìä Analytics dashboard
- üë• Team collaboration features
- üîç Advanced search with filters
- üì± Mobile responsive design
- üé® Theme customization
- üîê Advanced security (rate limiting, CAPTCHA)
- üìà Usage tracking and billing
- üåç Internationalization (i18n)

---

## License

This learning roadmap is provided as-is for educational purposes.

---

## Contributing

If you find improvements or want to share your experience, please open an issue or PR!

---

**Happy Learning! üöÄ**

Start with Phase 1 and build your knowledge step by step. Remember: every expert was once a beginner!

